from src.processing.cleaner import clean_text
from src.processing.deduper import dedupe_document
from src.processing.language_detector import detect_language
from src.chunking.chunker_engine import ChunkerEngine
from src.embedding.embedding_engine import EmbeddingEngine

# === Mock du client d'embedding ===
class MockEmbeddingClient:
    def get_embedding(self, text, model):
        # renvoie un vecteur simulÃ©, longueur = 1536 par dÃ©faut
        return [float(len(text) % 10)] * 1536

# === Exemple de texte trilingue ===
sample_text = """
ğŸ‡«ğŸ‡· Version entretien technique (FR) : Processing
Lâ€™Ã©tape de processing applique des transformations mÃ©tier aux textes validÃ©s pour garantir leur qualitÃ© avant le dÃ©coupage. Le pipeline inclut le nettoyage, la dÃ©tection de langue, la suppression des doublons et la validation du format. Chaque transformation est encapsulÃ©e dans un fichier dÃ©diÃ©, et un worker orchestre le flux : il consomme les messages, applique les traitements, puis publie le message enrichi. Cette architecture modulaire assure des donnÃ©es propres, enrichies et prÃªtes pour le chunking, tout en maintenant une sÃ©paration nette entre logique mÃ©tier et orchestration.

ğŸ‡«ğŸ‡· Version entretien technique â€“ Collecte & Extraction
AprÃ¨s la collecte, les donnÃ©es brutes sont stockÃ©es dans MinIO. Lâ€™Ã©tape dâ€™extraction sâ€™appuie sur des fichiers mÃ©tiers spÃ©cialisÃ©s pour traiter les PDF, les URLs et les rÃ©ponses API. Chaque contenu est transformÃ© en un schÃ©ma JSON homogÃ¨ne, puis rÃ©injectÃ© dans un bucket dÃ©diÃ© au processing. Lâ€™orchestration est assurÃ©e par extract_worker.py, qui hÃ©rite de BaseWorker pour gÃ©rer la rÃ©ception des messages via RabbitMQ, appeler les extracteurs mÃ©tiers, et publier les rÃ©sultats. Le tout est instrumentÃ© avec du logging structurÃ© et des mÃ©triques Prometheus, garantissant traÃ§abilitÃ©, observabilitÃ© et scalabilitÃ©.

ğŸ‡«ğŸ‡· RÃ©sumÃ© Embedding & Vector DB
Les chunks sont transformÃ©s en vecteurs numÃ©riques (embeddings) pour capturer leur sens, puis stockÃ©s dans une base vectorielle (Milvus) pour un accÃ¨s rapide et indexable. Chaque vecteur conserve ses mÃ©tadonnÃ©es (id, ordre, contenu, langue) et les opÃ©rations sont validÃ©es et surveillÃ©es via des mÃ©triques. Lâ€™orchestration (queue, retries, logs) reste sÃ©parÃ©e de la logique mÃ©tier.

Le Retrieval Layer transforme les requÃªtes utilisateurs en contexte structurÃ© pour le LLM. Il commence par classifier la requÃªte (lexicale, sÃ©mantique, factuelle, contextuelle) et oriente vers les retrievers appropriÃ©s (Milvus pour sÃ©mantique, Elasticsearch pour lexical, ou hybride). Les rÃ©sultats sont normalisÃ©s et fusionnÃ©s par le Ranker, puis raffinÃ©s par le Re-Ranker, qui peut utiliser diffÃ©rents modÃ¨les selon le type de requÃªte pour maximiser la pertinence. Enfin, le Context Builder agrÃ¨ge les chunks, gÃ¨re les mÃ©tadonnÃ©es et la limite de tokens pour produire un contexte exploitable par le LLM. Cette architecture garde la vector DB et le moteur lexical sÃ©parÃ©s, et prÃ©pare les donnÃ©es pour le repo LLM Integration sans lier les deux.
"""

def main():
    # Clean
    cleaned = clean_text(sample_text)
    print("âœ… Texte nettoyÃ© :")
    print(cleaned[:200], "...")  # aperÃ§u
    print("-" * 80)

    # Dedupe
    deduped = dedupe_document(cleaned)
    print("âœ… Texte dÃ©dupliquÃ© :")
    print(deduped[:200], "...")
    print("-" * 80)

    # Dtect language
    language = detect_language(deduped)
    print(f"âœ… Langue dÃ©tectÃ©e : {language}")
    print("-" * 80)

    # Chunking
    chunk_config = {
        "strategy": "by_tokens",  # ou "by_tokens", "by_paragraph"
        "max_tokens": 128,
        "overlap": 20
    }
    chunker = ChunkerEngine(chunk_config)
    chunks = chunker.chunk(deduped)

    print(f"âœ… Chunking terminÃ© ({len(chunks)} chunks produits) :")
    for i, chunk in enumerate(chunks[:5], start=1):
        print(f"Chunk {i}: {chunk['content'][:100]}...")
    print("-" * 80)

    # Embedding (mock)
    embedding_config = {
        "model": "mock-model",
        "dimension": 1536,
        "batch_size": 32
    }
    embedding_engine = EmbeddingEngine(embedding_config, model_client=MockEmbeddingClient())

    embeddings = []
    for chunk in chunks:
        content = chunk.get("content", "")
        vector = embedding_engine.embed(content)
        embeddings.append(vector)

    print(f"âœ… Embedding terminÃ© pour {len(embeddings)} chunks")
    for i, vec in enumerate(embeddings[:3], start=1):
        print(f"Chunk {i} vector (aperÃ§u 10 premiÃ¨res valeurs) : {vec[:10]}")

if __name__ == "__main__":
    main()